# -*- coding: utf-8 -*-
"""spam_anik_hamid_done.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X8S_Eq0KylTFHNxgGLmOOkh7EmVs6Jnq
"""

#  EDA(Exploratory data analysis) new thing??
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import string
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
nltk.download('stopwords')

# model train korar jonno lagbe
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from keras.callbacks import EarlyStopping, ReduceLROnPlateau

import warnings
warnings.filterwarnings('ignore')

!unzip dataset.zip

df = pd.read_csv('spam.csv' , encoding = 'latin-1')
df.head()

df.shape

sns.countplot(x='v2', data=df)
plt.show()

# 1. Data cleaning
df.drop( columns = ['Unnamed: 2','Unnamed: 3','Unnamed: 4'], inplace = True)

df

df.sample(3)

df.rename(columns = {'v1' :'text','v2':'target'}, inplace = True)

df

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

df['target']=encoder.fit_transform(df['target'])

df

ham_msg = df[df.target == 0]
spam_msg = df[df.target == 1]
ham_msg = ham_msg.sample(n=len(spam_msg), random_state=42)

# Concatenate the downsampled DataFrames
balanced_data = pd.concat([ham_msg, spam_msg]).reset_index(drop=True)

# Plot the counts of the balanced dataset
plt.figure(figsize=(8, 6))
sns.countplot(data=balanced_data, x='target')
plt.title('Distribution of Ham and Spam email messages after downsampling')
plt.xlabel('Message types')
plt.ylabel('Count')
plt.xticks(ticks=[0, 1], labels=['Ham', 'Spam'])
plt.show()

df

balanced_data['text'] = balanced_data['text'].str.replace('Subject', '')
balanced_data.head()

punctuations_list = string.punctuation
def remove_punctuations(text):
	# Convert the text to a string if it's not already a string
	text = str(text)
	temp = str.maketrans('', '', punctuations_list)
	return text.translate(temp)

balanced_data['text']= balanced_data['text'].apply(lambda x: remove_punctuations(x))
balanced_data.head()

def remove_stopwords(text):
	stop_words = stopwords.words('english')

	imp_words = []

	# Storing important words
	for word in str(text).split():
		word = word.lower()

		if word not in stop_words:
			imp_words.append(word)

	output = " ".join(imp_words)

	return output


balanced_data['text'] = balanced_data['text'].apply(lambda text: remove_stopwords(text))
balanced_data.head()

def plot_word_cloud(data, typ):
	email_corpus = " ".join(data['text'])

	plt.figure(figsize=(7, 7))

	wc = WordCloud(background_color='black',
				max_words=100,
				width=800,
				height=400,
				collocations=False).generate(email_corpus)

	plt.imshow(wc, interpolation='bilinear')
	plt.title(f'WordCloud for {typ} emails', fontsize=15)
	plt.axis('off')
	plt.show()

plot_word_cloud(balanced_data[balanced_data['target'] == 0], typ='Non-Spam')
plot_word_cloud(balanced_data[balanced_data['target'] == 1], typ='Spam')

#train test split
train_X, test_X, train_Y, test_Y = train_test_split(balanced_data['text'],
													balanced_data['target'],
													test_size = 0.2,
													random_state = 42)

# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(train_X)

# Convert text to sequences
train_sequences = tokenizer.texts_to_sequences(train_X)
test_sequences = tokenizer.texts_to_sequences(test_X)

# Pad sequences to have the same length
max_len = 100 # maximum sequence length
train_sequences = pad_sequences(train_sequences,
								maxlen=max_len,
								padding='post',
								truncating='post')
test_sequences = pad_sequences(test_sequences,
							maxlen=max_len,
							padding='post',
							truncating='post')

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Embedding(input_dim=len(tokenizer.word_index) + 1,
                                    output_dim=128,  # dimension barano hoieche
                                    input_length=max_len))
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)))
model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)))
model.add(tf.keras.layers.Dropout(0.5))  # 50% dropout added
model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))

# Print the model summary
model.summary()

model.compile(loss = tf.keras.losses.BinaryCrossentropy(from_logits = True),
			metrics = ['accuracy'],
			optimizer = 'adam')

es = EarlyStopping(patience=3,
				monitor = 'val_accuracy',
				restore_best_weights = True)

lr = ReduceLROnPlateau(patience = 2,
					monitor = 'val_loss',
					factor = 0.5,
					verbose = 0)

# Train the model
history = model.fit(train_sequences, train_Y,
					validation_data=(test_sequences, test_Y),
					epochs=20,
					batch_size=32,
					callbacks = [lr, es]
				)

# Evaluate the model
test_loss, test_accuracy = model.evaluate(test_sequences, test_Y)
print('Test Loss :',test_loss)
print('Test Accuracy :',test_accuracy)

plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()

from tensorflow.keras.models import load_model
import pickle


model = load_model('model.h5')


model_json = model.to_json()
with open('spam_model_architecture.json', 'w') as json_file:
    json_file.write(model_json)


weights = model.get_weights()
with open('model.pkl', 'wb') as file:
    pickle.dump(weights, file)

from sklearn.feature_extraction.text import TfidfVectorizer


texts = df['text'].astype(str).tolist()

#  TfidfVectorizer
tfidf_vectorizer = TfidfVectorizer()
tfidf_vectorizer.fit(texts)

print("TF-IDF vectorizer has been fitted successfully.")

import pickle

# Save
with open('vectorizer.pkl', 'wb') as file:
    pickle.dump(tfidf_vectorizer, file)

print("TF-IDF vectorizer has been saved successfully.")

import pickle

# Save
with open('vectorizer.pkl', 'wb') as file:
    pickle.dump(tfidf_vectorizer, file)

print("TF-IDF vectorizer has been saved successfully.")

#..................main part done above .........abyss

from google.colab import files

# Download the vectorizer file
files.download('vectorizer.pkl')

# Download the model file
files.download('model.pkl')

model.save('model.h5')  #  H5 file

import tensorflow as tf
import tensorflowjs as tfjs

# Convertion of model from h5 to tfjs
tfjs.converters.save_keras_model(model, 'tfjs_model')

import shutil

shutil.make_archive('tfjs_model', 'zip', 'tfjs_model')

from google.colab import files

files.download('tfjs_model.zip')